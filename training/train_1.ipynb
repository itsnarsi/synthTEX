{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set augmentation and Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Crop Augment\n",
    "class RandomCrop():\n",
    "    def __init__(self, min_crop = 256, max_crop = 1024):\n",
    "        self.min_crop = min_crop\n",
    "        self.max_crop = max_crop\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_crop(I1, crop_size):\n",
    "        h, w = I1.size # Assume I1.size == I2.size\n",
    "        th, tw = crop_size\n",
    "        \n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "\n",
    "        i = torch.randint(0, h - th + 1, size=(1, )).item()\n",
    "        j = torch.randint(0, w - tw + 1, size=(1, )).item()\n",
    "        return i, j, i+th, j+tw\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        cs = torch.randint(self.min_crop, self.max_crop, size=(1, )).item()\n",
    "        crop_size = (cs, cs)\n",
    "        bbox =  self.random_crop(I1, crop_size)\n",
    "        \n",
    "        return I1.crop(bbox), I2.crop(bbox)\n",
    "    \n",
    "class RandomRotate():\n",
    "    def __init__(self, min_angle = -25, max_angle = 25):\n",
    "        self.min_angle = min_angle\n",
    "        self.max_angle = max_angle\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        angle = torch.randint(self.min_angle, self.max_angle, size=(1, )).item()\n",
    "        return TF.rotate(I1, angle, Image.BILINEAR), TF.rotate(I2, angle, Image.BILINEAR)\n",
    "\n",
    "class RandomFlip():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        horizontal_flip = np.random.choice([True, False])\n",
    "        vertical_flip = np.random.choice([True, False])\n",
    "        if horizontal_flip:\n",
    "            I1 = I1.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            I2 = I2.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        if vertical_flip:\n",
    "            I1 = I1.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "            I2 = I2.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        \n",
    "        return I1, I2\n",
    "\n",
    "class ColorJitter():\n",
    "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "        self.color_jitter = transforms.ColorJitter(brightness, contrast, saturation, hue)\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        \n",
    "        return self.color_jitter(I1), I2\n",
    "\n",
    "class CenterCrop():\n",
    "    def __init__(self, crop):\n",
    "        self.center_crop = transforms.CenterCrop(crop)\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        return self.center_crop(I1), self.center_crop(I2)\n",
    "    \n",
    "\n",
    "class Resize():\n",
    "    def __init__(self, size):\n",
    "        self.resize = transforms.Resize(size)\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        return self.resize(I1), self.resize(I2)\n",
    "    \n",
    "class ToTensor():\n",
    "    def __init__(self):\n",
    "        self.tensor = transforms.ToTensor()\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        return self.tensor(I1), self.tensor(I2)\n",
    "    \n",
    "\n",
    "class Normalize():\n",
    "    def __init__(self, mean = (0.5,0.5, 0.5), std = (0.5,0.5, 0.5)):\n",
    "        self.norm = transforms.Normalize(mean, std)\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        return self.norm(I1), self.norm(I2)\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    RandomRotate(min_angle = -25, max_angle = 25),\n",
    "    RandomCrop(min_crop = 256, max_crop = 1024),\n",
    "    Resize(340),\n",
    "    CenterCrop(256),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.5),\n",
    "    RandomFlip(),\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,0.5, 0.5), (0.5,0.5, 0.5))\n",
    "])\n",
    "    \n",
    "class TextureDataset(Dataset):\n",
    "    def __init__(self, imgs, transform = data_transform):\n",
    "\n",
    "        self.imgs = imgs\n",
    "        self.num_samples = len(self.imgs)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        I = Image.open(self.imgs[i]).convert(\"RGB\")\n",
    "        I_diffuse = I.crop((0, 0, 1024, 1024)) \n",
    "        I_normal = I.crop((1024, 0, 2048, 1024)) \n",
    "        if self.transform:\n",
    "            I_diffuse, I_normal = self.transform([I_diffuse, I_normal])\n",
    "        return I_diffuse, I_normal\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextureLightDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_fldr: str = '', data_ext: str = '.png', batch_size: int = 64, train_val_split:float = 0.9, num_workers: int = 0):\n",
    "        super().__init__()\n",
    "        self.data_fldr = data_fldr\n",
    "        self.data_ext = data_ext\n",
    "        self.batch_size = batch_size\n",
    "        self.train_val_split = train_val_split\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.imgs = glob(self.data_fldr + os.sep + \"*\" + self.data_ext)\n",
    "        self.num_imgs = len(self.imgs)\n",
    "\n",
    "    def setup(self, stage_name):\n",
    "        np.random.shuffle(self.imgs)\n",
    "        \n",
    "        self.prepare_data()\n",
    "        \n",
    "        self.train_dataset = TextureDataset(self.imgs)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_ch = 3, out_ch = 32, ker = 3, stride = 1, groups = 1):\n",
    "\n",
    "        layers = []\n",
    "        pad = ker//2 if ker > 1 else 0\n",
    "        layers += [nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=ker, padding = pad, bias=True, stride = stride, groups = groups),\n",
    "                   nn.InstanceNorm2d(out_ch),\n",
    "                   nn.ReLU(True),\n",
    "                   ]\n",
    "\n",
    "        return layers\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, reshape = [6, 256, 256]):\n",
    "        super().__init__()\n",
    "        self.reshape = reshape\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return z.view(z.size(0), *self.reshape)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch = 3, out_ch = 32, ker = 3):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        pad = ker//2 if ker > 1 else 0\n",
    "        layers += [nn.Conv2d(in_channels=in_ch, out_channels=out_ch//2, kernel_size=ker, padding = pad, bias=False, stride = 1),\n",
    "                   nn.InstanceNorm2d(out_ch),\n",
    "                   nn.ReLU(True),\n",
    "                   nn.Conv2d(in_channels=out_ch//2, out_channels=out_ch, kernel_size=ker, padding = pad, bias=False, stride = 1)\n",
    "                   ]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.layers(z) + z\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_size):\n",
    "        super().__init__()\n",
    "        self.noise_size = noise_size\n",
    "        \n",
    "        self.noise_block = nn.Sequential(\n",
    "            *conv_block(in_ch = noise_size, out_ch = 512, ker = 1, stride = 1),\n",
    "            *conv_block(in_ch = 512, out_ch = 512, ker = 1, stride = 1),\n",
    "            Reshape([32, 4, 4]),\n",
    "            *conv_block(in_ch = 32, out_ch = 256, ker = 1, stride = 1),\n",
    "        )\n",
    "        \n",
    "        self.feat_block_1 = nn.Sequential(\n",
    "            ResBlock(in_ch = 256, out_ch = 256, ker = 3),\n",
    "            *conv_block(in_ch = 256, out_ch = 512, ker = 1, stride = 1),\n",
    "            nn.PixelShuffle(2), #8x8\n",
    "            *conv_block(in_ch = 128, out_ch = 256, ker = 1, stride = 1),\n",
    "        )\n",
    "        \n",
    "        self.feat_block_2 = nn.Sequential(\n",
    "            ResBlock(in_ch = 256, out_ch = 256, ker = 3),\n",
    "            *conv_block(in_ch = 256, out_ch = 512, ker = 1, stride = 1),\n",
    "            nn.PixelShuffle(2), #16x16\n",
    "            *conv_block(in_ch = 128, out_ch = 256, ker = 1, stride = 1),\n",
    "        )\n",
    "        \n",
    "        self.feat_block_3 = nn.Sequential(\n",
    "            ResBlock(in_ch = 256, out_ch = 256, ker = 3),\n",
    "            *conv_block(in_ch = 256, out_ch = 512, ker = 1, stride = 1),\n",
    "            nn.PixelShuffle(2), #32x32\n",
    "            *conv_block(in_ch = 128, out_ch = 256, ker = 1, stride = 1),\n",
    "        )\n",
    "        \n",
    "        self.feat_block_4 = nn.Sequential(\n",
    "            ResBlock(in_ch = 256, out_ch = 256, ker = 3),\n",
    "            *conv_block(in_ch = 256, out_ch = 512, ker = 1, stride = 1),\n",
    "            nn.PixelShuffle(2), #64x64\n",
    "            *conv_block(in_ch = 128, out_ch = 256, ker = 1, stride = 1),\n",
    "        )\n",
    "        \n",
    "        self.feat_block_5 = nn.Sequential(\n",
    "            ResBlock(in_ch = 256, out_ch = 256, ker = 3),\n",
    "            *conv_block(in_ch = 256, out_ch = 512, ker = 1, stride = 1),\n",
    "            nn.PixelShuffle(2), #128x128\n",
    "            *conv_block(in_ch = 128, out_ch = 256, ker = 1, stride = 1),\n",
    "        )\n",
    "        \n",
    "        self.feat_block_6 = nn.Sequential(\n",
    "            ResBlock(in_ch = 256, out_ch = 256, ker = 3),\n",
    "            *conv_block(in_ch = 256, out_ch = 512, ker = 1, stride = 1),\n",
    "            nn.PixelShuffle(2), #256x256\n",
    "            *conv_block(in_ch = 128, out_ch = 256, ker = 1, stride = 1),\n",
    "        )\n",
    "        \n",
    "        self.final_block = nn.Sequential(\n",
    "            ResBlock(in_ch = 256, out_ch = 256, ker = 3),\n",
    "            nn.Conv2d(in_channels=256, out_channels=6, kernel_size=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        noise_feat = self.noise_block(z)\n",
    "        \n",
    "        gen_feat_1 = self.feat_block_1(noise_feat)\n",
    "        gen_feat_2 = self.feat_block_2(gen_feat_1)\n",
    "        gen_feat_3 = self.feat_block_3(gen_feat_2)\n",
    "        gen_feat_4 = self.feat_block_4(gen_feat_3)\n",
    "        gen_feat_5 = self.feat_block_5(gen_feat_4)\n",
    "        gen_feat_6 = self.feat_block_6(gen_feat_5)\n",
    "        \n",
    "        return self.final_block(gen_feat_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *self.conv_block(in_ch = 6, out_ch = 32, ker = 3, stride = 2), # 128x128\n",
    "            *self.conv_block(in_ch = 32, out_ch = 64, ker = 3, stride = 2), # 64x64\n",
    "            *self.conv_block(in_ch = 64, out_ch = 128, ker = 3, stride = 2), # 32x32\n",
    "            *self.conv_block(in_ch = 128, out_ch = 128, ker = 3, stride = 2), # 16x16\n",
    "            *self.conv_block(in_ch = 128, out_ch = 256, ker = 3, stride = 2), # 8x8\n",
    "            nn.Conv2d(in_channels=256, out_channels=1, kernel_size=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def conv_block(in_ch = 3, out_ch = 32, ker = 3, stride = 1, groups = 1):\n",
    "\n",
    "        layers = []\n",
    "        pad = ker//2 if ker > 1 else 0\n",
    "        layers += [nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=ker, padding = pad, bias=True, stride = stride, groups = groups),\n",
    "                   nn.BatchNorm2d(out_ch),\n",
    "                   nn.LeakyReLU(0.2, True),\n",
    "                   ]\n",
    "\n",
    "        return layers\n",
    "        \n",
    "    def forward(self, I):\n",
    "        return self.model(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        width,\n",
    "        height,\n",
    "        latent_dim: int = 64,\n",
    "        lr: float = 0.0001,\n",
    "        b1: float = 0.5,\n",
    "        b2: float = 0.999,\n",
    "        batch_size: int = 32,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # networks\n",
    "        data_shape = [channels, width, height]\n",
    "        self.generator = Generator(noise_size=self.hparams.latent_dim)\n",
    "        self.discriminator = Discriminator()\n",
    "\n",
    "        self.validation_z = torch.randn(2, self.hparams.latent_dim, 1, 1)\n",
    "\n",
    "        self.example_input_array = torch.zeros(2, self.hparams.latent_dim, 1, 1)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        imgs_diffuse, imgs_normal = batch\n",
    "        imgs = torch.cat([imgs_diffuse, imgs_normal], dim = 1)\n",
    "\n",
    "        # sample noise\n",
    "        z = torch.randn(imgs.shape[0], self.hparams.latent_dim, 1, 1)\n",
    "        z = z.type_as(imgs)\n",
    "\n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "\n",
    "            # generate images\n",
    "            self.generated_imgs = self(z)\n",
    "\n",
    "            # adversarial loss\n",
    "            g_loss = - self.discriminator(self(z)).mean()\n",
    "            \n",
    "            self.log('g_loss', g_loss, prog_bar=True, on_step = True)\n",
    "            return g_loss\n",
    "\n",
    "        # train discriminator\n",
    "        if optimizer_idx == 1:\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "\n",
    "            real_pred = self.discriminator(imgs)\n",
    "            real_loss = torch.nn.ReLU()(1.0 - real_pred).mean()\n",
    "\n",
    "            fake_pred = self.discriminator(self(z).detach())\n",
    "            fake_loss = torch.nn.ReLU()(1.0 + fake_pred).mean()\n",
    "\n",
    "            # discriminator loss is the average of these\n",
    "            d_loss = real_loss + fake_loss\n",
    "            \n",
    "            self.log('d_loss', d_loss, prog_bar=True, on_step = True)\n",
    "            return d_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr*10.0, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr/10.0, betas=(b1, b2))\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        z = self.validation_z.type_as(self.generated_imgs)\n",
    "\n",
    "        # log sampled images\n",
    "        sample_imgs = self(z).detach()\n",
    "        sample_imgs = torch.cat([sample_imgs[:, :3, :, :], sample_imgs[:, 3:, :, :]], dim = 3)\n",
    "        grid = torchvision.utils.make_grid(sample_imgs)\n",
    "        self.logger.experiment.add_image('generated_images', grid, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fldr = r\"G:\\texture\\data_1\"\n",
    "root_dir = r\"E:\\SP_2021\\synthTEX_logging\"\n",
    "exp_name = \"model_exp_01\"\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type          | Params | In sizes      | Out sizes       \n",
      "-----------------------------------------------------------------------------------\n",
      "0 | generator     | Generator     | 5.4 M  | [2, 64, 1, 1] | [2, 6, 256, 256]\n",
      "1 | discriminator | Discriminator | 539 K  | ?             | ?               \n",
      "-----------------------------------------------------------------------------------\n",
      "6.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.0 M     Total params\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35e86885fd845f9b10d79973cc23a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training'), FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), maxâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narsi\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = TextureLightDataModule(data_fldr, batch_size = batch_size)\n",
    "model = GAN(6, 256, 256, batch_size = batch_size)\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=10, progress_bar_refresh_rate=1, default_root_dir = root_dir + os.sep + exp_name)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
