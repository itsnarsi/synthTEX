{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm.notebook import  tqdm\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "from PIL import Image\n",
    "\n",
    "import lpips \n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set augmentation and Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Crop Augment\n",
    "class RandomCrop():\n",
    "    def __init__(self, min_crop = 256, max_crop = 1024):\n",
    "        self.min_crop = min_crop\n",
    "        self.max_crop = max_crop\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_crop(I1, crop_size):\n",
    "        h, w = I1.size # Assume I1.size == I2.size\n",
    "        th, tw = crop_size\n",
    "        \n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "\n",
    "        i = torch.randint(0, h - th + 1, size=(1, )).item()\n",
    "        j = torch.randint(0, w - tw + 1, size=(1, )).item()\n",
    "        return i, j, i+th, j+tw\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        cs = torch.randint(self.min_crop, self.max_crop, size=(1, )).item()\n",
    "        crop_size = (cs, cs)\n",
    "        bbox =  self.random_crop(I1, crop_size)\n",
    "        \n",
    "        return I1.crop(bbox), I2.crop(bbox)\n",
    "    \n",
    "class RandomRotate():\n",
    "    def __init__(self, min_angle = -25, max_angle = 25):\n",
    "        self.min_angle = min_angle\n",
    "        self.max_angle = max_angle\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        angle = torch.randint(self.min_angle, self.max_angle, size=(1, )).item()\n",
    "        return TF.rotate(I1, angle, Image.BILINEAR), TF.rotate(I2, angle, Image.BILINEAR)\n",
    "\n",
    "class RandomFlip():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        horizontal_flip = np.random.choice([True, False])\n",
    "        vertical_flip = np.random.choice([True, False])\n",
    "        if horizontal_flip:\n",
    "            I1 = I1.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            I2 = I2.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        if vertical_flip:\n",
    "            I1 = I1.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "            I2 = I2.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        \n",
    "        return I1, I2\n",
    "\n",
    "class ColorJitter():\n",
    "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "        self.color_jitter = transforms.ColorJitter(brightness, contrast, saturation, hue)\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        \n",
    "        return self.color_jitter(I1), I2\n",
    "\n",
    "class CenterCrop():\n",
    "    def __init__(self, crop):\n",
    "        self.center_crop = transforms.CenterCrop(crop)\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        return self.center_crop(I1), self.center_crop(I2)\n",
    "    \n",
    "\n",
    "class Resize():\n",
    "    def __init__(self, size):\n",
    "        self.resize = transforms.Resize(size)\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        return self.resize(I1), self.resize(I2)\n",
    "    \n",
    "class ToTensor():\n",
    "    def __init__(self):\n",
    "        self.tensor = transforms.ToTensor()\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        return self.tensor(I1), self.tensor(I2)\n",
    "    \n",
    "\n",
    "class Normalize():\n",
    "    def __init__(self, mean = (0.5,0.5, 0.5), std = (0.5,0.5, 0.5)):\n",
    "        self.norm = transforms.Normalize(mean, std)\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        return self.norm(I1), self.norm(I2)\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "#     RandomCrop(min_crop = 128, max_crop = 1024),\n",
    "#     Resize(160),\n",
    "#     RandomRotate(min_angle = -25, max_angle = 25),\n",
    "#     CenterCrop(128),\n",
    "#     ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.5),\n",
    "#     RandomFlip(),\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,0.5, 0.5), (0.5,0.5, 0.5))\n",
    "])\n",
    "    \n",
    "class TextureDataset(Dataset):\n",
    "    def __init__(self, imgs, img_size = 128, transform = data_transform):\n",
    "\n",
    "        self.imgs = imgs\n",
    "        self.img_size = img_size\n",
    "        self.num_samples = len(self.imgs)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        I = Image.open(self.imgs[i]).convert(\"RGB\")\n",
    "        I_diffuse = I.crop((0, 0, self.img_size, self.img_size)) \n",
    "        I_normal = I.crop((self.img_size, 0, self.img_size*2, self.img_size)) \n",
    "        if self.transform:\n",
    "            I_diffuse, I_normal = self.transform([I_diffuse, I_normal])\n",
    "        return I_diffuse, I_normal\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "src_fldr = r\"G:\\texture\\data_1\"\n",
    "aug_data = r\"G:\\texture\\augmented_data\"\n",
    "\n",
    "imgs = glob(src_fldr + os.sep + \"*.png\")\n",
    "texture_dataset = TextureDataset(imgs)\n",
    "\n",
    "c = 0\n",
    "for i in tqdm(range(len(imgs))):\n",
    "    I = Image.open(imgs[i]).convert(\"RGB\")\n",
    "    I_diffuse = I.crop((0, 0, 1024, 1024)) \n",
    "    I_normal = I.crop((1024, 0, 2048, 1024)) \n",
    "    for j in range(1):\n",
    "        I_diffuse_1, I_normal_1 = data_transform([I_diffuse, I_normal])\n",
    "        I_1 = np.concatenate([np.uint8(I_diffuse_1).copy(), np.uint8(I_normal_1).copy()], axis = 1)\n",
    "        Image.fromarray(I_1).save(aug_data + os.sep + \"{0:04d}.png\".format(c))\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiable Augmentation for Data-Efficient GAN Training\n",
    "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
    "# https://arxiv.org/pdf/2006.10738\n",
    "def DiffAugment(x, policy=['color', 'translation', 'cutout'], channels_first=True):\n",
    "    if policy:\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        for p in policy:\n",
    "            for f in AUGMENT_FNS[p]:\n",
    "                x = f(x)\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "        x = x.contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_brightness(x):\n",
    "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_saturation(x):\n",
    "    x_mean = x.mean(dim=1, keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_contrast(x):\n",
    "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_translation(x, ratio=0.125):\n",
    "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
    "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
    "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
    "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
    "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
    "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
    "    mask[grid_batch, grid_x, grid_y] = 0\n",
    "    x = x * mask.unsqueeze(1)\n",
    "    return x\n",
    "\n",
    "AUGMENT_FNS = {\n",
    "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "    'translation': [rand_translation],\n",
    "    'cutout': [rand_cutout],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextureLightDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_fldr: str = '', data_ext: str = '.png', batch_size: int = 64, train_val_split:float = 0.9, num_workers: int = 0):\n",
    "        super().__init__()\n",
    "        self.data_fldr = data_fldr\n",
    "        self.data_ext = data_ext\n",
    "        self.batch_size = batch_size\n",
    "        self.train_val_split = train_val_split\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.imgs = glob(self.data_fldr + os.sep + \"*\" + self.data_ext)\n",
    "        self.num_imgs = len(self.imgs)\n",
    "\n",
    "    def setup(self, stage_name):\n",
    "        np.random.shuffle(self.imgs)\n",
    "        \n",
    "        self.prepare_data()\n",
    "        \n",
    "        self.train_dataset = TextureDataset(self.imgs)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, reshape = [6, 128, 128]):\n",
    "        super().__init__()\n",
    "        self.reshape = reshape\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return z.view(z.size(0), *self.reshape)\n",
    "\n",
    "class NoiseInjection(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.weight = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "    def forward(self, feat, noise=None):\n",
    "        if noise is None:\n",
    "            batch, _, height, width = feat.shape\n",
    "            noise = torch.randn(batch, 1, height, width).to(feat.device)\n",
    "\n",
    "        return feat + self.weight * noise\n",
    "    \n",
    "class InputNoiseLayer(nn.Module):\n",
    "    def __init__(self, noise_size, channel, size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.init = nn.Sequential(\n",
    "            nn.ConvTranspose2d(noise_size, channel, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(channel), \n",
    "            nn.SiLU(True) \n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.init(z)\n",
    "        \n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_size):\n",
    "        super().__init__()\n",
    "        self.noise_size = noise_size\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            InputNoiseLayer(noise_size, 1024, 4),\n",
    "            *self.conv_block(in_ch = 1024, out_ch = 1024, ker = 3, stride = 1),\n",
    "            *self.conv_block(in_ch = 1024, out_ch = 512, ker = 3, stride = 1),\n",
    "            nn.Upsample(scale_factor = 2), # 8x8\n",
    "            *self.conv_block(in_ch = 512, out_ch = 512, ker = 3, stride = 1),\n",
    "            *self.conv_block(in_ch = 512, out_ch = 256, ker = 3, stride = 1),\n",
    "            nn.Upsample(scale_factor = 2), # 16x16\n",
    "            *self.conv_block(in_ch = 256, out_ch = 256, ker = 3, stride = 1),\n",
    "            *self.conv_block(in_ch = 256, out_ch = 128, ker = 3, stride = 1),\n",
    "            nn.Upsample(scale_factor = 2), # 32x32\n",
    "            *self.conv_block(in_ch = 128, out_ch = 128, ker = 3, stride = 1),\n",
    "            *self.conv_block(in_ch = 128, out_ch = 64, ker = 3, stride = 1),\n",
    "            nn.Upsample(scale_factor = 2), # 64x64\n",
    "            *self.conv_block(in_ch = 64, out_ch = 64, ker = 3, stride = 1),\n",
    "            *self.conv_block(in_ch = 64, out_ch = 64, ker = 3, stride = 1),\n",
    "            nn.Upsample(scale_factor = 2), # 128x128\n",
    "            *self.conv_block(in_ch = 64, out_ch = 64, ker = 3, stride = 1),\n",
    "            *self.conv_block(in_ch = 64, out_ch = 64, ker = 3, stride = 1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=1),\n",
    "            nn.Hardtanh()\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def conv_block(in_ch = 3, out_ch = 32, ker = 3, stride = 1, groups = 1):\n",
    "\n",
    "        layers = []\n",
    "        pad = ker//2 if ker > 1 else 0\n",
    "        if pad > 0:\n",
    "            layers = [nn.ReflectionPad2d(pad)]\n",
    "        layers += [nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=ker, padding = 0, bias=True, stride = stride, groups = groups),\n",
    "                   NoiseInjection(),\n",
    "                   nn.BatchNorm2d(out_ch),\n",
    "                   nn.SiLU(True),\n",
    "                   ]\n",
    "\n",
    "        return layers        \n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.feat_1 = nn.Sequential(\n",
    "            *self.conv_block(in_ch = 3, out_ch = 32, ker = 3, stride = 2), # 64x64\n",
    "            *self.conv_block(in_ch = 32, out_ch = 64, ker = 3, stride = 2), # 32x32\n",
    "        )\n",
    "        \n",
    "        self.feat_2 = nn.Sequential(\n",
    "            *self.conv_block(in_ch = 64, out_ch = 128, ker = 3, stride = 2), # 16x16\n",
    "            *self.conv_block(in_ch = 128, out_ch = 128, ker = 3, stride = 2), # 8x8\n",
    "        )\n",
    "        \n",
    "        self.pred = nn.Sequential(\n",
    "            *self.conv_block(in_ch = 128, out_ch = 256, ker = 3, stride = 2), # 4x4\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Conv2d(in_channels=256, out_channels=1, kernel_size=1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        self.recon_1 = nn.Sequential(\n",
    "            *self.up_conv_block(in_ch = 64, out_ch = 64, ker = 3),\n",
    "            *self.up_conv_block(in_ch = 64, out_ch = 64, ker = 3),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3),\n",
    "            nn.Hardtanh()\n",
    "        )\n",
    "        \n",
    "        self.recon_2 = nn.Sequential(\n",
    "            *self.up_conv_block(in_ch = 128, out_ch = 64, ker = 3),\n",
    "            *self.up_conv_block(in_ch = 64, out_ch = 64, ker = 3),\n",
    "            *self.up_conv_block(in_ch = 64, out_ch = 64, ker = 3),\n",
    "            *self.up_conv_block(in_ch = 64, out_ch = 64, ker = 3),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(in_channels=64, out_channels=3, kernel_size=3),\n",
    "            nn.Hardtanh()\n",
    "        )\n",
    "        \n",
    "#         self.model = nn.Sequential(\n",
    "#             *self.conv_block(in_ch = 3, out_ch = 32, ker = 3, stride = 2), # 64x64\n",
    "#             *self.conv_block(in_ch = 32, out_ch = 64, ker = 3, stride = 2), # 32x32\n",
    "#             *self.conv_block(in_ch = 64, out_ch = 128, ker = 3, stride = 2), # 16x16\n",
    "#             *self.conv_block(in_ch = 128, out_ch = 128, ker = 3, stride = 2), # 8x8\n",
    "#             *self.conv_block(in_ch = 128, out_ch = 256, ker = 3, stride = 2), # 4x4\n",
    "#             nn.Dropout(0.25),\n",
    "#             nn.Conv2d(in_channels=256, out_channels=1, kernel_size=1),\n",
    "#             nn.Flatten()\n",
    "#         )\n",
    "\n",
    "    @staticmethod\n",
    "    def up_conv_block(in_ch = 3, out_ch = 32, ker = 3, stride = 1, groups = 1):\n",
    "\n",
    "        layers = [nn.Upsample(scale_factor = 2)]\n",
    "        pad = ker//2 if ker > 1 else 0\n",
    "        if pad > 0:\n",
    "            layers += [nn.ReflectionPad2d(pad)]\n",
    "        layers += [nn.utils.spectral_norm(nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=ker, padding = 0, bias=True, stride = stride, groups = groups)),\n",
    "                   NoiseInjection(),\n",
    "                   nn.BatchNorm2d(out_ch),\n",
    "                   nn.SiLU(True),\n",
    "                   ]\n",
    "        return layers\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def conv_block(in_ch = 3, out_ch = 32, ker = 3, stride = 1, groups = 1):\n",
    "\n",
    "        layers = []\n",
    "        pad = ker//2 if ker > 1 else 0\n",
    "        if pad > 0:\n",
    "            layers = [nn.ReflectionPad2d(pad)]\n",
    "        layers += [nn.utils.spectral_norm(nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=ker, padding = 0, bias=True, stride = stride, groups = groups)),\n",
    "                   nn.BatchNorm2d(out_ch),\n",
    "                   nn.LeakyReLU(0.2, True),\n",
    "                   ]\n",
    "\n",
    "        return layers\n",
    "        \n",
    "    def forward(self, I):\n",
    "        \n",
    "        feat_1 = self.feat_1(I)\n",
    "        feat_2 = self.feat_2(feat_1)\n",
    "        \n",
    "        I_32x32 = self.recon_1(feat_1)\n",
    "        I_8x8 = self.recon_2(feat_2)\n",
    "        \n",
    "        pred = self.pred(feat_2)\n",
    "        \n",
    "        return pred, I_32x32, I_8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        try:\n",
    "            m.weight.data.normal_(0.0, 0.02)\n",
    "        except:\n",
    "            pass\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "class GAN(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        width,\n",
    "        height,\n",
    "        latent_dim: int = 64,\n",
    "        lr: float = 0.0001,\n",
    "        b1: float = 0.5,\n",
    "        b2: float = 0.9,\n",
    "        batch_size: int = 32,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # networks\n",
    "        data_shape = [channels, width, height]\n",
    "        self.generator = Generator(noise_size=self.hparams.latent_dim)\n",
    "        self.discriminator = Discriminator()\n",
    "        \n",
    "        self.generator.apply(weights_init)\n",
    "        self.discriminator.apply(weights_init)\n",
    "        \n",
    "        self.percept = lpips.PerceptualLoss(model='net-lin', net='vgg', use_gpu=True)\n",
    "    \n",
    "        self.validation_z = torch.randn(4, self.hparams.latent_dim, 1, 1)\n",
    "\n",
    "        self.example_input_array = torch.zeros(2, self.hparams.latent_dim, 1, 1)\n",
    "        \n",
    "        self.epoch_end_count = 0\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        imgs, imgs_normal = batch#_diffuse\n",
    "#         plt.figure(figsize = (10, 10))\n",
    "#         plt.imshow(torchvision.utils.make_grid((imgs+1)/2.0, nrow = 4).detach().cpu().numpy().transpose(1, 2, 0))\n",
    "#         plt.show()\n",
    "#         imgs = torch.cat([imgs_diffuse, imgs_normal], dim = 1)\n",
    "#         imgs = torch.cat([DiffAugment(x) for x in torch.chunk(imgs, imgs.size(0), dim = 0)]).contiguous()\n",
    "#         imgs = DiffAugment(imgs)\n",
    "        # sample noise\n",
    "        z = torch.randn(imgs.shape[0], self.hparams.latent_dim, 1, 1)\n",
    "        z = z.type_as(imgs)\n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "\n",
    "            # generate images\n",
    "            self.generated_imgs = self(z)\n",
    "\n",
    "            # adversarial loss\n",
    "            g_loss, _, _ = self.discriminator(self.generated_imgs)\n",
    "            g_loss = - g_loss.mean()\n",
    "            \n",
    "            self.log('g_loss', g_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True)\n",
    "            return g_loss\n",
    "\n",
    "        # train discriminator\n",
    "        if optimizer_idx == 1:\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "            \n",
    "            real_pred, real_I_32x32, real_I_8x8 = self.discriminator(imgs)\n",
    "            real_disc_loss = torch.nn.ReLU()(torch.rand_like(real_pred) * 0.2 + 0.8  - real_pred).mean()\n",
    "            #nn.functional.interpolate(imgs, size = (32, 32), mode = \"bilinear\", align_corners=False)\n",
    "            recon_loss_32x32 = self.percept(real_I_32x32, imgs).mean()\n",
    "            recon_loss_8x8 = self.percept(real_I_8x8, imgs).mean()\n",
    "            recon_loss = (recon_loss_32x32 + recon_loss_8x8)\n",
    "            real_loss = real_disc_loss + recon_loss\n",
    "            \n",
    "            fake_imgs = self(z).detach()\n",
    "#             fake_imgs = DiffAugment(fake_imgs)#torch.cat([DiffAugment(x) for x in torch.chunk(fake_imgs, fake_imgs.size(0), dim = 0)]).contiguous()\n",
    "            fake_pred, _, _ = self.discriminator(fake_imgs)\n",
    "            fake_loss = torch.nn.ReLU()(torch.rand_like(fake_pred) * 0.2 + 0.8  + fake_pred).mean()\n",
    "\n",
    "            # discriminator loss is the average of these\n",
    "            d_loss = real_loss + fake_loss\n",
    "            \n",
    "            self.log('d_loss', d_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True)\n",
    "            self.log('recon_loss', recon_loss, on_step=True, on_epoch=False, prog_bar=True, logger=True)\n",
    "            return d_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        z = self.validation_z.type_as(self.generated_imgs)\n",
    "\n",
    "        # log sampled images\n",
    "        sample_imgs = self(z).detach() + 1.0\n",
    "#         sample_imgs = torch.cat([sample_imgs[:, :3, :, :], sample_imgs[:, 3:, :, :]], dim = 3)+1.0\n",
    "        grid = torchvision.utils.make_grid(sample_imgs/2.0)\n",
    "        self.logger.experiment.add_image('generated_images', grid, self.epoch_end_count)\n",
    "        self.epoch_end_count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fldr = r\"G:\\texture\\augmented_data\"\n",
    "root_dir = r\"E:\\SP_2021\\synthTEX_logging\"\n",
    "exp_name = \"model_exp_02\"\n",
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = TextureLightDataModule(data_fldr, batch_size = batch_size)\n",
    "model = GAN(3, 128, 128, batch_size = batch_size)\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=100, progress_bar_refresh_rate=1, default_root_dir = root_dir + os.sep + exp_name)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAN.load_from_checkpoint(r\"E:\\SP_2021\\synthTEX_logging\\model_exp_02\\lightning_logs\\version_0\\checkpoints\\epoch=0-step=5523.ckpt\", channels = 3, width = 128, height = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_normal(size, threshold = 2):\n",
    "    values = truncnorm.rvs(-threshold, threshold, size = size)\n",
    "    return torch.from_numpy(np.float32(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = truncated_normal((16, 64, 1, 1), threshold = 1.0)\n",
    "I = model.generator(z)\n",
    "I = (I + 1)/2.0\n",
    "grid = torchvision.utils.make_grid(I, nrow = 4).data.cpu().numpy()\n",
    "grid = np.uint8(grid * 255.0).transpose(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
