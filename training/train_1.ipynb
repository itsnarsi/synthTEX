{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import ArgumentParser\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set augmentation and Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Crop Augment\n",
    "class RandomCrop():\n",
    "    def __init__(self, min_crop = 256, max_crop = 1024):\n",
    "        self.min_crop = min_crop\n",
    "        self.max_crop = max_crop\n",
    "    \n",
    "    @staticmethod\n",
    "    def random_crop(I1, crop_size):\n",
    "        h, w = I1.size # Assume I1.size == I2.size\n",
    "        th, tw = crop_size\n",
    "        \n",
    "        if w == tw and h == th:\n",
    "            return 0, 0, h, w\n",
    "\n",
    "        i = torch.randint(0, h - th + 1, size=(1, )).item()\n",
    "        j = torch.randint(0, w - tw + 1, size=(1, )).item()\n",
    "        return i, j, i+th, j+tw\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        cs = torch.randint(self.min_crop, self.max_crop, size=(1, )).item()\n",
    "        crop_size = (cs, cs)\n",
    "        bbox =  self.random_crop(I1, crop_size)\n",
    "        \n",
    "        return I1.crop(bbox), I2.crop(bbox)\n",
    "    \n",
    "class RandomRotate():\n",
    "    def __init__(self, min_angle = -25, max_angle = 25):\n",
    "        self.min_angle = min_angle\n",
    "        self.max_angle = max_angle\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        angle = torch.randint(self.min_angle, self.max_angle, size=(1, )).item()\n",
    "        return TF.rotate(I1, angle, Image.BILINEAR), TF.rotate(I2, angle, Image.BILINEAR)\n",
    "\n",
    "class RandomFlip():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        horizontal_flip = np.random.choice([True, False])\n",
    "        vertical_flip = np.random.choice([True, False])\n",
    "        if horizontal_flip:\n",
    "            I1 = I1.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            I2 = I2.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        if vertical_flip:\n",
    "            I1 = I1.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "            I2 = I2.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "        \n",
    "        return I1, I2\n",
    "\n",
    "class ColorJitter():\n",
    "    def __init__(self, brightness=0, contrast=0, saturation=0, hue=0):\n",
    "        self.color_jitter = transforms.ColorJitter(brightness, contrast, saturation, hue)\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        \n",
    "        return self.color_jitter(I1), I2\n",
    "\n",
    "class CenterCrop():\n",
    "    def __init__(self, crop):\n",
    "        self.center_crop = transforms.CenterCrop(crop)\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        return self.center_crop(I1), self.center_crop(I2)\n",
    "    \n",
    "\n",
    "class Resize():\n",
    "    def __init__(self, size):\n",
    "        self.resize = transforms.Resize(size)\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        return self.resize(I1), self.resize(I2)\n",
    "    \n",
    "class ToTensor():\n",
    "    def __init__(self):\n",
    "        self.tensor = transforms.ToTensor()\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        return self.tensor(I1), self.tensor(I2)\n",
    "    \n",
    "\n",
    "class Normalize():\n",
    "    def __init__(self, mean = (0.5,0.5, 0.5), std = (0.5,0.5, 0.5)):\n",
    "        self.norm = transforms.Normalize(mean, std)\n",
    "    \n",
    "    def __call__(self, img_set):\n",
    "        I1, I2 = img_set\n",
    "        return self.norm(I1), self.norm(I2)\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    RandomRotate(min_angle = -25, max_angle = 25),\n",
    "    RandomCrop(min_crop = 128, max_crop = 1024),\n",
    "    Resize(160),\n",
    "    CenterCrop(128),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.5),\n",
    "    RandomFlip(),\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,0.5, 0.5), (0.5,0.5, 0.5))\n",
    "])\n",
    "    \n",
    "class TextureDataset(Dataset):\n",
    "    def __init__(self, imgs, transform = data_transform):\n",
    "\n",
    "        self.imgs = imgs\n",
    "        self.num_samples = len(self.imgs)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        I = Image.open(self.imgs[i]).convert(\"RGB\")\n",
    "        I_diffuse = I.crop((0, 0, 1024, 1024)) \n",
    "        I_normal = I.crop((1024, 0, 2048, 1024)) \n",
    "        if self.transform:\n",
    "            I_diffuse, I_normal = self.transform([I_diffuse, I_normal])\n",
    "        return I_diffuse, I_normal\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextureLightDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, data_fldr: str = '', data_ext: str = '.png', batch_size: int = 64, train_val_split:float = 0.9, num_workers: int = 0):\n",
    "        super().__init__()\n",
    "        self.data_fldr = data_fldr\n",
    "        self.data_ext = data_ext\n",
    "        self.batch_size = batch_size\n",
    "        self.train_val_split = train_val_split\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        self.imgs = glob(self.data_fldr + os.sep + \"*\" + self.data_ext)\n",
    "        self.num_imgs = len(self.imgs)\n",
    "\n",
    "    def setup(self, stage_name):\n",
    "        np.random.shuffle(self.imgs)\n",
    "        \n",
    "        self.prepare_data()\n",
    "        \n",
    "        self.train_dataset = TextureDataset(self.imgs)\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reshape(nn.Module):\n",
    "    def __init__(self, reshape = [6, 128, 128]):\n",
    "        super().__init__()\n",
    "        self.reshape = reshape\n",
    "    \n",
    "    def forward(self, z):\n",
    "        return z.view(z.size(0), *self.reshape)\n",
    "\n",
    "class InputNoiseLayer(nn.Module):\n",
    "    def __init__(self, noise_size, channel, size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.init = nn.Sequential(\n",
    "            nn.utils.spectral_norm(nn.ConvTranspose2d(noise_size, channel, 4, 1, 0, bias=False)),\n",
    "            nn.BatchNorm2d(channel), \n",
    "            nn.SiLU(True) \n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.init(z)\n",
    "        \n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_size):\n",
    "        super().__init__()\n",
    "        self.noise_size = noise_size\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            InputNoiseLayer(noise_size, 512, 4),\n",
    "            *self.conv_block(in_ch = 512, out_ch = 512, ker = 3, stride = 1),\n",
    "            nn.Upsample(scale_factor = 2), # 8x8\n",
    "            *self.conv_block(in_ch = 512, out_ch = 256, ker = 3, stride = 1),\n",
    "            *self.conv_block(in_ch = 256, out_ch = 256, ker = 3, stride = 1),\n",
    "            nn.Upsample(scale_factor = 2), # 16x16\n",
    "            *self.conv_block(in_ch = 256, out_ch = 128, ker = 3, stride = 1),\n",
    "            *self.conv_block(in_ch = 128, out_ch = 128, ker = 3, stride = 1),\n",
    "            nn.Upsample(scale_factor = 2), # 32x32\n",
    "            *self.conv_block(in_ch = 128, out_ch = 64, ker = 3, stride = 1),\n",
    "            *self.conv_block(in_ch = 64, out_ch = 64, ker = 3, stride = 1),\n",
    "            nn.Upsample(scale_factor = 2), # 64x64\n",
    "            *self.conv_block(in_ch = 64, out_ch = 32, ker = 3, stride = 1),\n",
    "            *self.conv_block(in_ch = 32, out_ch = 32, ker = 3, stride = 1),\n",
    "            nn.Upsample(scale_factor = 2), # 128x128\n",
    "            *self.conv_block(in_ch = 32, out_ch = 32, ker = 3, stride = 1),\n",
    "            nn.Conv2d(in_channels=32, out_channels=6, kernel_size=1),\n",
    "            nn.Hardtanh()\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def conv_block(in_ch = 3, out_ch = 32, ker = 3, stride = 1, groups = 1):\n",
    "\n",
    "        layers = []\n",
    "        pad = ker//2 if ker > 1 else 0\n",
    "        if pad > 0:\n",
    "            layers = [nn.ReflectionPad2d(pad)]\n",
    "        layers += [nn.utils.spectral_norm(nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=ker, padding = 0, bias=True, stride = stride, groups = groups)),\n",
    "                   nn.BatchNorm2d(out_ch),\n",
    "                   nn.SiLU(True),\n",
    "                   ]\n",
    "\n",
    "        return layers        \n",
    "    \n",
    "    def forward(self, z):\n",
    "        return self.model(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *self.conv_block(in_ch = 6, out_ch = 32, ker = 3, stride = 2), # 64x64\n",
    "            *self.conv_block(in_ch = 32, out_ch = 64, ker = 3, stride = 2), # 32x32\n",
    "            *self.conv_block(in_ch = 64, out_ch = 128, ker = 3, stride = 2), # 16x16\n",
    "            *self.conv_block(in_ch = 128, out_ch = 128, ker = 3, stride = 2), # 8x8\n",
    "            *self.conv_block(in_ch = 128, out_ch = 256, ker = 3, stride = 2), # 4x4\n",
    "            nn.Conv2d(in_channels=256, out_channels=1, kernel_size=1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def conv_block(in_ch = 3, out_ch = 32, ker = 3, stride = 1, groups = 1):\n",
    "\n",
    "        layers = []\n",
    "        pad = ker//2 if ker > 1 else 0\n",
    "        if pad > 0:\n",
    "            layers = [nn.ReflectionPad2d(pad)]\n",
    "        layers += [nn.utils.spectral_norm(nn.Conv2d(in_channels=in_ch, out_channels=out_ch, kernel_size=ker, padding = 0, bias=True, stride = stride, groups = groups)),\n",
    "                   nn.BatchNorm2d(out_ch),\n",
    "                   nn.LeakyReLU(0.1, True),\n",
    "                   ]\n",
    "\n",
    "        return layers\n",
    "        \n",
    "    def forward(self, I):\n",
    "        return self.model(I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        try:\n",
    "            m.weight.data.normal_(0.0, 0.02)\n",
    "        except:\n",
    "            pass\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "class GAN(pl.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        width,\n",
    "        height,\n",
    "        latent_dim: int = 64,\n",
    "        lr: float = 0.0001,\n",
    "        b1: float = 0.5,\n",
    "        b2: float = 0.999,\n",
    "        batch_size: int = 32,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # networks\n",
    "        data_shape = [channels, width, height]\n",
    "        self.generator = Generator(noise_size=self.hparams.latent_dim)\n",
    "        self.discriminator = Discriminator()\n",
    "        \n",
    "        self.generator.apply(weights_init)\n",
    "        self.discriminator.apply(weights_init)\n",
    "\n",
    "        self.validation_z = torch.randn(2, self.hparams.latent_dim, 1, 1)\n",
    "\n",
    "        self.example_input_array = torch.zeros(2, self.hparams.latent_dim, 1, 1)\n",
    "        \n",
    "        self.epoch_end_count = 0\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "        imgs_diffuse, imgs_normal = batch\n",
    "        imgs = torch.cat([imgs_diffuse, imgs_normal], dim = 1)\n",
    "\n",
    "        # sample noise\n",
    "        z = torch.randn(imgs.shape[0], self.hparams.latent_dim, 1, 1)\n",
    "        z = z.type_as(imgs)\n",
    "        # train generator\n",
    "        if optimizer_idx == 0:\n",
    "\n",
    "            # generate images\n",
    "            self.generated_imgs = self(z)\n",
    "\n",
    "            # adversarial loss\n",
    "            g_loss = - self.discriminator(self.generated_imgs).mean()\n",
    "            \n",
    "            self.log('g_loss', g_loss, prog_bar=True, on_step = True)\n",
    "            return g_loss\n",
    "\n",
    "        # train discriminator\n",
    "        if optimizer_idx == 1:\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "\n",
    "            real_pred = self.discriminator(imgs)\n",
    "            real_loss = torch.nn.ReLU()(1.0 - real_pred).mean()\n",
    "\n",
    "            fake_pred = self.discriminator(self(z).detach())\n",
    "            fake_loss = torch.nn.ReLU()(1.0 + fake_pred).mean()\n",
    "\n",
    "            # discriminator loss is the average of these\n",
    "            d_loss = real_loss + fake_loss\n",
    "            \n",
    "            self.log('d_loss', d_loss, prog_bar=True, on_step = True)\n",
    "            return d_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "\n",
    "        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "        return [opt_g, opt_d], []\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        z = self.validation_z.type_as(self.generated_imgs)\n",
    "\n",
    "        # log sampled images\n",
    "        sample_imgs = self(z).detach()\n",
    "        sample_imgs = torch.cat([sample_imgs[:, :3, :, :], sample_imgs[:, 3:, :, :]], dim = 3)+1.0\n",
    "        grid = torchvision.utils.make_grid(sample_imgs/2.0)\n",
    "        self.logger.experiment.add_image('generated_images', grid, self.epoch_end_count)\n",
    "        self.epoch_end_count += 1\n",
    "        print(\"PEPOWAVE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fldr = r\"G:\\texture\\data_1\"\n",
    "root_dir = r\"E:\\SP_2021\\synthTEX_logging\"\n",
    "exp_name = \"model_exp_01\"\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = TextureLightDataModule(data_fldr, batch_size = batch_size)\n",
    "model = GAN(6, 128, 128, batch_size = batch_size)\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=1000, progress_bar_refresh_rate=1, default_root_dir = root_dir + os.sep + exp_name)\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
